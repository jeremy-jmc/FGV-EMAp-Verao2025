{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Direction Method of Multipliers (ADMM)\n",
    "\n",
    "O objetivo deste tutorial é ilustrar a criação de estruturas para implementar um algoritmo de otimização, que usa modelos `JuMP` como parte de um algoritmo maior.\n",
    "Nós vamos usar o [ADMM](https://en.wikipedia.org/wiki/Alternating_direction_method_of_multipliers) para resolver um problema de regressão linear com regularização $L^1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pacotes necessários\n",
    "\n",
    "Este notebook requer os seguintes pacotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "import Ipopt\n",
    "import HiGHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O caso clássico para ADMM é o de **otimização composta**, ou seja, problemas da forma\n",
    "$$ \\min_{x,z} f(x) + g(z) \\quad \\text{sujeito a} \\quad z = Tx.$$\n",
    "\n",
    "Estes problemas aparecem em diversas aplicações, e o interesse deles é duplo:\n",
    "- A função objetivo pode ser separada em duas partes, uma que depende apenas de $x$ e outra que depende apenas de $z$;\n",
    "- Cada uma das funções $f$ e $g$ pode ser otimizada de forma eficiente, mas, em geral, usando algoritmos diferentes.\n",
    "\n",
    "Alguns exemplos de problemas compostos são:\n",
    "- Regressão linear com regularização $L^1$ (Compressed Sensing, LASSO, Total Variation Denoising, etc.):\n",
    "$$ \\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1$$\n",
    "- Problemas de máxima veroossimilhança com uma priori: como $p(\\theta|\\text{data}) \\propto \\log p(\\text{data}|\\theta) + \\log p(\\theta)$, temos (ignorando uma constante)\n",
    "$$ \\max_\\theta \\log p(\\text{data}|\\theta) + \\log p(\\theta) $$\n",
    "- Problemas de otimização estocástica em dois estágios:\n",
    "$$ \\min_x f_1(x) + \\mathbb{E}[f_2(x,\\xi)] $$\n",
    "- Cálculo de interseções:\n",
    "$$ x \\in C_1 \\quad \\text{e} \\quad x \\in C_2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O ADMM é um método iterativo para abordar problemas desta forma, e é baseado na seguinte ideia:\n",
    "- O problema acima é equivalente a\n",
    "$$ \\min_{x,z} f(x) + g(z) + \\frac{\\rho}{2} \\|z - Tx\\|_2^2 \\quad \\text{sujeito a} \\quad z = Tx,$$\n",
    "onde $\\rho > 0$ é um parâmetro.\n",
    "- Dualizando a restrição com o multiplicador de Lagrange $\\lambda = (\\lambda_1, \\ldots, \\lambda_n)$ correto, obtemos\n",
    "$$ \\min_{x,z} f(x) + g(z) + \\frac{\\rho}{2} \\|z - Tx\\|_2^2 + \\lambda^T(z - Tx).$$\n",
    "Este problema é conhecido como **problema de Lagrangiano Aumentado**.\n",
    "\n",
    "- A partir de uma condição inicial $(x_0, z_0, \\lambda_0)$, o ADMM itera os seguintes passos:\n",
    "$$ \\begin{align*}\n",
    "x^{k+1} &:= \\arg\\min_x f(x) + \\frac{\\rho}{2} \\|z^k - Tx\\|_2^2 + \\lambda^T(z^k - Tx) \\\\\n",
    "z^{k+1} &:= \\arg\\min_z g(z) + \\frac{\\rho}{2} \\|z - Tx^{k+1}\\|_2^2 + \\lambda^T(z - Tx^{k+1}) \\\\\n",
    "\\lambda^{k+1} &:= \\lambda^k + \\rho (z^{k+1} - Tx^{k+1}).\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos construir uma interface de forma que o usuário passe\n",
    "- dois modelos JuMP, `f` e `g`, que representam as funções $f$ e $g$,\n",
    "- dois vetores de variáveis, `x` e `z`, para representar as variáveis de decisão $x$ e $z$,\n",
    "- a matriz `T`, e\n",
    "- o parâmetro `ρ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "augmented_lagrangian! (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function augmented_lagrangian!(f::JuMP.Model, x, T, ρ; first::Bool)\n",
    "    m,n = size(T)\n",
    "\n",
    "    # Modificando f:\n",
    "    if first\n",
    "        other = @variable(f, [1:m])\n",
    "        error = @expression(f, other - T*x)\n",
    "    else\n",
    "        other = @variable(f, [1:n])\n",
    "        error = @expression(f, x - T*other)\n",
    "    end\n",
    "\n",
    "    # Save information for ADMM\n",
    "    f.ext[:orig_obj] = objective_function(f)\n",
    "    f.ext[:ρ] = ρ\n",
    "    f.ext[:var] = x\n",
    "    f.ext[:other] = other\n",
    "    f.ext[:error] = error\n",
    "\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_pair! (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_pair!(f, g, x, z, T, ρ)\n",
    "    augmented_lagrangian!(f, x, T, ρ, first=true)\n",
    "    augmented_lagrangian!(g, z, T, ρ, first=false)\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve_proximal (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function solve_proximal(m::JuMP.Model, λᵏ, otherᵏ)\n",
    "    ρ = m.ext[:ρ]\n",
    "    error = m.ext[:error]\n",
    "    fix.(m.ext[:other], otherᵏ)\n",
    "    @objective(m, Min, m.ext[:orig_obj] + 0.5ρ * sum(error.^2) + sum(λᵏ .* error))\n",
    "    optimize!(m)\n",
    "    return value.(m.ext[:var])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(0)\n",
    "\n",
    "A = randn(3, 10)\n",
    "b = randn(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Model(Ipopt.Optimizer)\n",
    "set_silent(f)\n",
    "@variable(f, x[1:10])\n",
    "@objective(f, Min, sum( (A*x .- b).^2 ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Model(HiGHS.Optimizer)\n",
    "set_silent(g)\n",
    "@variable(g, z[1:10])\n",
    "@variable(g, absz[1:10])\n",
    "@constraint(g, absz .>= z)\n",
    "@constraint(g, absz .>= -z)\n",
    "@objective(g, Min, sum( absz ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "I10 = 1.0 * I(10)\n",
    "ρ = 1.0\n",
    "make_pair!(f, g, f[:x], g[:z], I10, ρ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       "  0.1805764074366593\n",
       "  0.35809697231813414\n",
       "  0.12016691252601668\n",
       "  0.15509628615117643\n",
       " -0.11240045494422057\n",
       " -0.30632299651558326\n",
       " -0.32079953593884103\n",
       " -0.08762970853491206\n",
       " -0.08139288517599796\n",
       " -0.2512313905384395"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_next = solve_proximal(f, zeros(10), zeros(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_next = solve_proximal(g, zeros(10), x_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.6972353886899763\n",
      "2: 0.6436953452629165\n",
      "3: 0.5910537152121168\n",
      "4: 0.3659829118449364\n",
      "5: 0.26458376821572105\n",
      "6: 0.24151275335881317\n",
      "7: 0.15751403191216995\n",
      "8: 0.04609751545708216\n",
      "9: 0.07416003477687477\n",
      "10: 0.08037853103147241\n",
      "11: 0.07975683412398091\n",
      "12: 0.07170738998717716\n",
      "13: 0.0567043406472091\n",
      "14: 0.03836400142643656\n",
      "15: 0.020442907268347618\n",
      "16: 0.012738675955534294\n",
      "17: 0.014149740237798487\n",
      "18: 0.021287569983069823\n",
      "19: 0.03067791463466744\n",
      "20: 0.024763969974625964\n"
     ]
    }
   ],
   "source": [
    "l_curr = zeros(10)\n",
    "for i in 1:20\n",
    "    curr_err = (I10*x_next - z_next)\n",
    "    println(\"$i: $(norm(curr_err))\")\n",
    "    l_curr .-= ρ * curr_err\n",
    "    x_next = solve_proximal(f, l_curr, z_next)\n",
    "    z_next = solve_proximal(g, l_curr, x_next)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×3 Matrix{Float64}:\n",
       "  0.0113319    0.0       -0.331667\n",
       "  0.524761     0.524761  -1.0\n",
       "  0.167878     0.167878  -1.0\n",
       "  0.00603121   0.0       -0.594444\n",
       " -0.00694999   0.0        0.484254\n",
       " -0.621835    -0.621835   1.0\n",
       "  0.00344862   0.0        0.965942\n",
       "  0.0137036    0.0        0.43191\n",
       " -0.0021465    0.0        0.289382\n",
       " -9.7338e-5    0.0        0.978745"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x_next z_next l_curr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando em uma função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "admm (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function admm(f, g, x, z, T, ρ; max_iter=100, tol=1e-6, verbose=0)\n",
    "    make_pair!(f, g, x, z, T, ρ)\n",
    "    λᵏ = zeros(length(z))\n",
    "    zᵏ = zeros(length(z))\n",
    "    for k in 1:max_iter\n",
    "        xᵏ = solve_proximal(f, λᵏ, zᵏ)\n",
    "        zᵏ = solve_proximal(g, λᵏ, xᵏ)\n",
    "        errᵏ = T*xᵏ - zᵏ\n",
    "        λᵏ .-= ρ * errᵏ\n",
    "        if verbose > 0\n",
    "            print(\"$k: $(norm(errᵏ))\")\n",
    "            if verbose > 1\n",
    "                print(\"\\n  x = $xᵏ\\n  z = $zᵏ\\n  λ = $λᵏ\")\n",
    "            end\n",
    "            println()\n",
    "        end\n",
    "        if norm(errᵏ) < tol\n",
    "            return (x=xᵏ, z=zᵏ, λ=λᵏ, k=k, err=norm(errᵏ))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Model(Ipopt.Optimizer)\n",
    "set_silent(f)\n",
    "@variable(f, x[1:10])\n",
    "@objective(f, Min, sum( (A*x .- b).^2 ) )\n",
    "\n",
    "g = Model(HiGHS.Optimizer)\n",
    "set_silent(g)\n",
    "@variable(g, z[1:10])\n",
    "@variable(g, absz[1:10])\n",
    "@constraint(g, absz .>= z)\n",
    "@constraint(g, absz .>= -z)\n",
    "@objective(g, Min, sum( absz ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×3 Matrix{Float64}:\n",
       " -2.09582e-7   0.0       -0.343166\n",
       "  0.496928     0.496928  -1.0\n",
       "  0.155346     0.155346  -1.0\n",
       " -3.43461e-8   0.0       -0.601688\n",
       " -4.73088e-8   0.0        0.491978\n",
       " -0.663624    -0.663624   1.0\n",
       "  4.7395e-7    0.0        0.965803\n",
       "  2.07584e-7   0.0        0.420656\n",
       "  4.74858e-8   0.0        0.292202\n",
       "  1.86819e-7   0.0        0.981713"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = admm(f, g, f[:x], g[:z], I10, ρ; max_iter=100, tol=1e-6, verbose=0)\n",
    "[ret.x ret.z ret.λ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 5.934794947901097e-7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.k, ret.err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma estrutura para armazenar o problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ADMM\n",
    "    f :: JuMP.Model\n",
    "    g :: JuMP.Model\n",
    "    x :: Vector{JuMP.VariableRef}\n",
    "    z :: Vector{JuMP.VariableRef}\n",
    "    T :: Matrix{Float64}\n",
    "    ρ :: Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "- Em vez de modificar os modelos de $f$ e $g$, crie uma cópia usando `Base.copy`.\n",
    "  Você também terá que usar `JuMP.set_optimizer` para dar um solver para cada cópia,\n",
    "  pois `Base.copy` não copia o solver.\n",
    "- Crie uma estrutura de dados `admm_data` para armazenar os dados do ADMM.\n",
    "  Esta estrutura deve conter os seguintes campos:\n",
    "  - `x`: o valor atual de $x$;\n",
    "  - `z`: o valor atual de $z$;\n",
    "  - `λ`: o valor atual de $\\lambda$;\n",
    "  - `ρ`: o valor atual de $\\rho$;\n",
    "  - `f`: o modelo de $f$;\n",
    "  - `g`: o modelo de $g$;\n",
    "  - `A`: a matriz $A$;\n",
    "  - `solver_f`: o solver usado para resolver o problema de $f$;\n",
    "  - `solver_g`: o solver usado para resolver o problema de $g$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
